services:

  ollama:
    image: ollama/ollama:latest  # Replace with the correct Ollama image or build if needed
    container_name: ollama_service
    ports:
      - "11434:11434"  # Ollama will serve on this port
    env_file: ".env"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - OLLAMA_API_URL=ollama_service:11434
      - OLLAMA_HOST=ollama_service:11434
      - OLLAMA_ORIGINS=http://ollama_service:11434
    volumes:
        # - ./ollama/ollama:/root/.ollama
        - ./entrypoint.sh:/entrypoint.sh
    runtime: nvidia
    pull_policy: always
    tty: true
    restart: always
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - chatbot_network  # Same network as the chatbot for communication
    # entrypoint: |
    #   bash -c "ollama run llama3.1 && ollama serve"
    entrypoint: ["/usr/bin/bash", "/entrypoint.sh"]

  chatbot:
    image: myllm:latest
    container_name: chatbot_app
    ports:
      - "8501:8501"
    env_file: ".env"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - OLLAMA_HOST=ollama_service:11434
      - OLLAMA_ORIGINS=http://ollama_service:11434
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - .:/app
    stdin_open: true
    tty: true
    depends_on:
      - ollama  # Ensure Ollama starts before the chatbot
    networks:
      - chatbot_network  # Assign both containers to the same network
    entrypoint: |
      /bin/bash -c "until curl -s http://ollama_service:11434 > /dev/null; do echo 'Waiting for Ollama...'; sleep 3; done; streamlit run ui.py"

networks:
  chatbot_network:
    driver: bridge