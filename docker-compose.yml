services:

  ollama:
    image: ollama/ollama:latest  # Replace with the correct Ollama image or build if needed
    container_name: ollama_service
    ports:
      - "11435:11434"  # Ollama will serve on this port
    env_file: ".env"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - OLLAMA_API_URL=ollama_service:11434
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - chatbot_network  # Same network as the chatbot for communication
    entrypoint: |
      bash -c "ollama serve"

  chatbot:
    image: myllm:latest
    container_name: chatbot_app
    ports:
      - "8501:8501"
    env_file: ".env"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_ORIGINS=http://0.0.0.0:11434
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - .:/app
    stdin_open: true
    tty: true
    depends_on:
      - ollama  # Ensure Ollama starts before the chatbot
    networks:
      - chatbot_network  # Assign both containers to the same network
    entrypoint: |
      /bin/bash -c "until curl -s http://ollama_service:11435 > /dev/null; do echo 'Waiting for Ollama...'; sleep 3; done; streamlit run ui.py"

networks:
  chatbot_network:
    driver: bridge